{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a94f946",
   "metadata": {},
   "source": [
    "# Hackathon Language Identification Challenge: Notebook\n",
    "\n",
    "## Introduction\n",
    "Welcome to the Language Identification Challenge Hackathon! In this challenge, we aim to build a robust language identification model that can accurately classify text into its respective language category. This notebook serves as a comprehensive guide to our approach, methodology, and the steps taken to create an effective language identification solution.\n",
    "\n",
    "## Challenge Overview\n",
    "Language identification is a crucial task in natural language processing (NLP) and has numerous applications, ranging from content filtering to improving machine translation systems. The goal of this hackathon is to leverage machine learning techniques to build a model that excels at accurately determining the language of a given text, even in cases of multilingual or ambiguous content.\n",
    "\n",
    "## Dataset\n",
    "Our dataset comprises a diverse collection of text samples from various languages. Each text entry is labeled with its corresponding language, forming the basis for supervised learning. The challenge is to train a classification model that can generalize well to unseen text data.\n",
    "\n",
    "## Approach\n",
    "\n",
    "#### Data Exploration:\n",
    "I will begin by exploring the dataset, gaining insights into its structure, and understanding the distribution of languages.\n",
    "\n",
    "#### Data Preprocessing: \n",
    "To prepare the data for model training, I will perform necessary preprocessing steps such as tokenization, handling missing values, and converting text into a suitable format for machine learning.\n",
    "\n",
    "#### Feature Engineering: \n",
    "Extracting relevant features is crucial for the success of our model. We may consider techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings.\n",
    "\n",
    "#### Model Selection: \n",
    "I will experiment with various classification algorithms, such as logistic regression, support vector machines, or neural networks, to identify the one that performs best for our specific language identification task.\n",
    "\n",
    "#### Model Training: \n",
    "Once the model is selected, I will train it on the training dataset and fine-tune hyperparameters to achieve optimal performance.\n",
    "\n",
    "#### Evaluation: \n",
    "We will evaluate the model using appropriate metrics, considering factors like precision, recall, and F1-score, given the potential class imbalance.\n",
    "\n",
    "#### Inference: \n",
    "After training the model, we will demonstrate its language identification capabilities on new, unseen text samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f479ab",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05e79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing of required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1246a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Training Data\n",
    "train_df = pd.read_csv('train_set.csv')\n",
    "\n",
    "#Importing the test data\n",
    "test_df = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c7908",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f774d924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xho</td>\n",
       "      <td>umgaqo-siseko wenza amalungiselelo kumaziko ax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xho</td>\n",
       "      <td>i-dha iya kuba nobulumko bokubeka umsebenzi na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eng</td>\n",
       "      <td>the province of kwazulu-natal department of tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nso</td>\n",
       "      <td>o netefatša gore o ba file dilo ka moka tše le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ven</td>\n",
       "      <td>khomishini ya ndinganyiso ya mbeu yo ewa maana...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang_id                                               text\n",
       "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...\n",
       "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...\n",
       "2     eng  the province of kwazulu-natal department of tr...\n",
       "3     nso  o netefatša gore o ba file dilo ka moka tše le...\n",
       "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f3b566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86594438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " There are 33000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print(f' There are {train_df.shape[0]} rows and {train_df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "211997bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang_id    object\n",
       "text       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4be57b6",
   "metadata": {},
   "source": [
    "# *Observations*\n",
    "#### **The dataset has the following columns**:\n",
    "\n",
    "laung_id: Represents the different types of language identifiction abbreviations.\n",
    "\n",
    "text: Contains the text of the sentences associated with each language.\n",
    "\n",
    "#### **Data Types**\n",
    "\n",
    "The data columns have the following data types:\n",
    "laung_id : strings (str) message: text (str) \n",
    "\n",
    "#### **Dataset Size**\n",
    "\n",
    "The dataset consists of 33000 entries\n",
    "\n",
    "This dataset will be used for training and evaluating machine learning models to classify which language the text column is in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cf9801",
   "metadata": {},
   "source": [
    "# **Observing the Target Variable**\n",
    "\n",
    "We will explore the following:\n",
    "<ul>\n",
    "  <li>Summary Statistics</li>\n",
    "  <li>Target Variable Distribution</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf5107d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 33000\n",
       "unique                                                29948\n",
       "top       ngokwesekhtjheni yomthetho ophathelene nalokhu...\n",
       "freq                                                     17\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explore summary Statistics\n",
    "train_df['text'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e98ab80",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "217514b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset:\n",
      "  lang_id                                               text\n",
      "0     xho  umgaqo-siseko wenza amalungiselelo kumaziko ax...\n",
      "1     xho  i-dha iya kuba nobulumko bokubeka umsebenzi na...\n",
      "2     eng  the province of kwazulu-natal department of tr...\n",
      "3     nso  o netefatša gore o ba file dilo ka moka tše le...\n",
      "4     ven  khomishini ya ndinganyiso ya mbeu yo ewa maana...\n",
      "\n",
      "Test Dataset:\n",
      "   index                                               text\n",
      "0      1  Mmasepala, fa maemo a a kgethegileng a letlele...\n",
      "1      2  Uzakwaziswa ngokufaneleko nakungafuneka eminye...\n",
      "2      3         Tshivhumbeo tshi fana na ngano dza vhathu.\n",
      "3      4  Kube inja nelikati betingevakala kutsi titsini...\n",
      "4      5                      Winste op buitelandse valuta.\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a60ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dbdd98b",
   "metadata": {},
   "source": [
    "# 5. Text Data Preprocessing \n",
    "\n",
    "#### Pre-processing is a crucial step in building language models as it helps prepare the raw text data for effective learning. The specific pre-processing steps depend on the nature of your language model and the task at hand. \n",
    "\n",
    "The following defines functions preprocess_lemmatize and preprocess_stemming and are used to clean and preprocess text data in the 'message' column of a DataFrame. It includes steps such as converting text to lowercase and usernames, expanding contractions, and lemmatizing the text. The processed DataFrames (lemmatized_train_df) can be used for training classification models on language identification analysis or other natural language processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d25f31cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, test_df):\n",
    "    # Initializing the TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fitting the vectorizer on the training data\n",
    "    vectorizer.fit(train_df['text'])\n",
    "\n",
    "    # Transforming the training and test data using the fitted vectorizer\n",
    "    train_features = vectorizer.transform(train_df['text'])\n",
    "    test_features = vectorizer.transform(test_df['text'])\n",
    "\n",
    "    return train_features, test_features, vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592a3ce",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d22af8ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_features, test_features, vectorizer \u001b[38;5;241m=\u001b[39m preprocess_data(train_df, test_df)\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(train_df, test_df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(train_df, test_df):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Initializing the TF-IDF vectorizer\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Fitting the vectorizer on the training data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     vectorizer\u001b[38;5;241m.\u001b[39mfit(train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "train_features, test_features, vectorizer = preprocess_data(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab682669",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743c51b",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9333355",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(train_features, train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang_id\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      2\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m      3\u001b[0m lr_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_features' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_df['lang_id'], test_size=0.2, random_state=42)\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_preds = lr_model.predict(X_val)\n",
    "lr_f1 = f1_score(y_val, lr_preds, average='weighted')\n",
    "\n",
    "print(\"Logistic Regression F1 Score:\", lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c1900",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc75a8a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m knn_model \u001b[38;5;241m=\u001b[39m KNeighborsClassifier()\n\u001b[1;32m----> 2\u001b[0m knn_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m knn_preds \u001b[38;5;241m=\u001b[39m knn_model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      4\u001b[0m knn_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val, knn_preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_preds = knn_model.predict(X_val)\n",
    "knn_f1 = f1_score(y_val, knn_preds, average='weighted')\n",
    "\n",
    "print(\"KNN F1 Score:\", knn_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4222683",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ea05d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m svm \u001b[38;5;241m=\u001b[39m SVC()\n\u001b[1;32m----> 2\u001b[0m svm\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m svm_predictions \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      4\u001b[0m svm_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val, svm_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svm_predictions = svm.predict(X_val)\n",
    "svm_f1 = f1_score(y_val, svm_predictions, average='weighted')\n",
    "print(\"SVM F1 Score:\", svm_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc4ebe",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1724c5cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nb \u001b[38;5;241m=\u001b[39m MultinomialNB()\n\u001b[1;32m----> 2\u001b[0m nb\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      3\u001b[0m nb_predictions \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m      4\u001b[0m nb_f1 \u001b[38;5;241m=\u001b[39m f1_score(y_val, nb_predictions, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "nb_predictions = nb.predict(X_val)\n",
    "nb_f1 = f1_score(y_val, nb_predictions, average='weighted')\n",
    "print(\"Naive Bayes F1 Score:\", nb_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922bb61",
   "metadata": {},
   "source": [
    "# Generate predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ece69c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Converting the test data into TF-IDF vectors\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_test \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generating predictions on the best performing model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m nb\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Converting the test data into TF-IDF vectors\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Generating predictions on the best performing model\n",
    "test_predictions = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f6aba",
   "metadata": {},
   "source": [
    "# Creating a csv for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ded759e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Creating a submission dataframe with 'index' and 'lang_id' columns\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m: test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang_id\u001b[39m\u001b[38;5;124m'\u001b[39m: test_predictions})\n\u001b[0;32m      4\u001b[0m submission_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinalSub1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating a submission dataframe with 'index' and 'lang_id' columns\n",
    "submission_df = pd.DataFrame({'index': test_data['index'], 'lang_id': test_predictions})\n",
    "\n",
    "submission_df.to_csv('FinalSub1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39f89f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799d6e5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c956af7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9bdbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677089e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23ad95e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e622a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "ad193818",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1ce35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27185a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
